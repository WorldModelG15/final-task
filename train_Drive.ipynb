{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_Drive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WorldModelG15/final-task/blob/mano/train_Drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "永続化処理追加"
      ],
      "metadata": {
        "id": "k5GUHe4GAKGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtSQoKJ3H6PL",
        "outputId": "92d15fc1-b6e7-4840-9cf3-ac195c7429ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#初回のみ\n",
        "#%cd /content/drive/MyDrive/\n",
        "#%mkdir dreamer_tasks\n",
        "#%cd /content/drive/MyDrive/dreamer_tasks\n",
        "#!git clone --recursive -b takata https://github.com/WorldModelG15/final-task.git\n",
        "#永続化\n",
        "#%cd final-task\n",
        "#%cd gym-duckietown/\n",
        "#!pip install -e . --target  /content/drive/MyDrive/dreamer_tasks/final-task/colab_modules/\n",
        "#!pip install pyvirtualdisplay --target /content/drive/MyDrive/dreamer_tasks/final-task/colab_modules/"
      ],
      "metadata": {
        "id": "u9zob2rwIFp_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append ('/content/drive/MyDrive/dreamer_tasks/final-task/colab_modules/')\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!apt-get install x11-utils -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-QqqWhGJtPu",
        "outputId": "b4fb0d49-0649-403b-f211-a02ed3be4d28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "def create_display():\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "    if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "        !sh xvfb start\n",
        "        %env DISPLAY=:1\n",
        "create_display()"
      ],
      "metadata": {
        "id": "iAMgXohY2P70"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/dreamer_tasks/final-task/\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/dreamer_tasks/final-task/gym-duckietown/src/')\n",
        "from utils.env import launch_env\n",
        "from utils.wrappers import (\n",
        "    EarlyStopWrapper,\n",
        "    NormalizeWrapper,\n",
        "    ImgWrapper,\n",
        "    DtRewardWrapper,\n",
        "    ActionWrapper,\n",
        "    ResizeWrapper,\n",
        "    EarlyStopWrapper,\n",
        ")\n",
        "from dreamer.config import DreamerConfig\n",
        "from dreamer.trainer import Trainer\n",
        "import torch\n",
        "\n",
        "env = launch_env(map_name=\"loop_obstacles\")#\"loop_pedestrians\")\n",
        "env = ResizeWrapper(env)\n",
        "# env = NormalizeWrapper(env)\n",
        "env = ImgWrapper(env)  # to make the images from 120x160x3 into 3x120x160\n",
        "env = ActionWrapper(env)\n",
        "env = DtRewardWrapper(env)\n",
        "env = EarlyStopWrapper(env)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "config = DreamerConfig()\n",
        "trainer = Trainer(env, device, config)\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXzAfRTIFQo8",
        "outputId": "f2fd45c2-53aa-4b19-935d-656ce0973022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dreamer_tasks/final-task\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:commons:version: 6.2.4 *\n",
            "DEBUG:typing:version: 6.2.3\n",
            "DEBUG:duckietown_world:duckietown-world version 6.2.38 path /content/drive/MyDrive/dreamer_tasks/final-task/colab_modules\n",
            "DEBUG:geometry:PyGeometry-z6 version 2.1.4 path /content/drive/MyDrive/dreamer_tasks/final-task/colab_modules\n",
            "DEBUG:aido_schemas:aido-protocols version 6.0.59 path /content/drive/MyDrive/dreamer_tasks/final-task/colab_modules\n",
            "DEBUG:nodes:version 6.2.13 path /content/drive/MyDrive/dreamer_tasks/final-task/colab_modules pyparsing 3.0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': ('directsound', 'openal', 'pulse', 'silent'), 'debug_font': False, 'debug_gl': True, 'debug_gl_trace': False, 'debug_gl_trace_args': False, 'debug_graphics_batch': False, 'debug_lib': False, 'debug_media': False, 'debug_texture': False, 'debug_trace': False, 'debug_trace_args': False, 'debug_trace_depth': 1, 'debug_trace_flush': True, 'debug_win32': False, 'debug_x11': False, 'graphics_vbo': True, 'shadow_window': True, 'vsync': None, 'xsync': True, 'xlib_fullscreen_override_redirect': False, 'darwin_cocoa': True, 'search_local_libs': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:duckietown_world: data: /content/drive/MyDrive/dreamer_tasks/final-task/colab_modules/duckietown_world/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode [   6/ 300] is collected. Total reward is -723.082793\n",
            "elasped time for interaction: 49.87s\n",
            "update_step:   1 model loss: 3100.92944, kl_loss: 3.00000, obs_loss: 3097.54053, reward_loss: 0.03900, collision_loss: 0.34980, value_loss: 0.00227 action_loss: 0.01897\n",
            "update_step:   2 model loss: 2982.34497, kl_loss: 3.00000, obs_loss: 2978.95923, reward_loss: 0.03619, collision_loss: 0.34961, value_loss: 0.00516 action_loss: -0.04544\n",
            "update_step:   3 model loss: 3035.30811, kl_loss: 3.00000, obs_loss: 3031.93701, reward_loss: 0.02179, collision_loss: 0.34942, value_loss: 0.01162 action_loss: -0.10703\n",
            "update_step:   4 model loss: 2996.79639, kl_loss: 3.00000, obs_loss: 2993.39526, reward_loss: 0.05204, collision_loss: 0.34916, value_loss: 0.02322 action_loss: -0.17694\n",
            "update_step:   5 model loss: 2984.77637, kl_loss: 3.00000, obs_loss: 2981.39136, reward_loss: 0.03555, collision_loss: 0.34935, value_loss: 0.03983 action_loss: -0.25073\n",
            "update_step:   6 model loss: 2878.13965, kl_loss: 3.00000, obs_loss: 2874.76953, reward_loss: 0.02085, collision_loss: 0.34925, value_loss: 0.05988 action_loss: -0.32363\n",
            "update_step:   7 model loss: 2942.40015, kl_loss: 3.00000, obs_loss: 2939.00342, reward_loss: 0.04784, collision_loss: 0.34886, value_loss: 0.08685 action_loss: -0.40397\n",
            "update_step:   8 model loss: 2892.49414, kl_loss: 3.00000, obs_loss: 2889.11646, reward_loss: 0.02927, collision_loss: 0.34850, value_loss: 0.11558 action_loss: -0.48191\n",
            "update_step:   9 model loss: 2620.06836, kl_loss: 3.00000, obs_loss: 2616.69629, reward_loss: 0.02354, collision_loss: 0.34866, value_loss: 0.14738 action_loss: -0.56201\n",
            "update_step:  10 model loss: 2431.97168, kl_loss: 3.00000, obs_loss: 2428.58813, reward_loss: 0.03504, collision_loss: 0.34847, value_loss: 0.18848 action_loss: -0.65635\n",
            "update_step:  11 model loss: 2731.73169, kl_loss: 3.00000, obs_loss: 2728.36011, reward_loss: 0.02332, collision_loss: 0.34821, value_loss: 0.21232 action_loss: -0.72001\n",
            "update_step:  12 model loss: 2412.10059, kl_loss: 3.00000, obs_loss: 2408.72119, reward_loss: 0.03127, collision_loss: 0.34824, value_loss: 0.23404 action_loss: -0.76633\n",
            "update_step:  13 model loss: 2233.39795, kl_loss: 3.00000, obs_loss: 2230.03857, reward_loss: 0.01087, collision_loss: 0.34836, value_loss: 0.24629 action_loss: -0.80288\n",
            "update_step:  14 model loss: 2238.11792, kl_loss: 3.00000, obs_loss: 2234.73706, reward_loss: 0.03215, collision_loss: 0.34854, value_loss: 0.26212 action_loss: -0.84781\n",
            "update_step:  15 model loss: 2204.07202, kl_loss: 3.00000, obs_loss: 2200.67773, reward_loss: 0.04563, collision_loss: 0.34853, value_loss: 0.28358 action_loss: -0.90259\n",
            "update_step:  16 model loss: 2086.09790, kl_loss: 3.00000, obs_loss: 2082.71655, reward_loss: 0.03305, collision_loss: 0.34831, value_loss: 0.31048 action_loss: -0.96925\n",
            "update_step:  17 model loss: 2022.90137, kl_loss: 3.00000, obs_loss: 2019.52661, reward_loss: 0.02642, collision_loss: 0.34841, value_loss: 0.32988 action_loss: -1.03065\n",
            "update_step:  18 model loss: 2004.12976, kl_loss: 3.00000, obs_loss: 2000.75647, reward_loss: 0.02550, collision_loss: 0.34773, value_loss: 0.33650 action_loss: -1.07524\n",
            "update_step:  19 model loss: 2043.89771, kl_loss: 3.00000, obs_loss: 2040.52051, reward_loss: 0.02943, collision_loss: 0.34782, value_loss: 0.33413 action_loss: -1.10415\n",
            "update_step:  20 model loss: 1837.88220, kl_loss: 3.00000, obs_loss: 1834.50500, reward_loss: 0.02904, collision_loss: 0.34809, value_loss: 0.33123 action_loss: -1.13016\n",
            "update_step:  21 model loss: 1853.58508, kl_loss: 3.00000, obs_loss: 1850.21790, reward_loss: 0.01907, collision_loss: 0.34810, value_loss: 0.31980 action_loss: -1.15266\n",
            "update_step:  22 model loss: 1850.95190, kl_loss: 3.00000, obs_loss: 1847.57129, reward_loss: 0.03186, collision_loss: 0.34874, value_loss: 0.30885 action_loss: -1.18156\n",
            "update_step:  23 model loss: 1778.96716, kl_loss: 3.00000, obs_loss: 1775.58423, reward_loss: 0.03440, collision_loss: 0.34852, value_loss: 0.30626 action_loss: -1.22884\n",
            "update_step:  24 model loss: 1758.02295, kl_loss: 3.00000, obs_loss: 1754.64917, reward_loss: 0.02513, collision_loss: 0.34860, value_loss: 0.29991 action_loss: -1.27495\n",
            "update_step:  25 model loss: 1726.80334, kl_loss: 3.00000, obs_loss: 1723.41943, reward_loss: 0.03551, collision_loss: 0.34836, value_loss: 0.29246 action_loss: -1.31576\n",
            "update_step:  26 model loss: 1683.53711, kl_loss: 3.00000, obs_loss: 1680.16309, reward_loss: 0.02536, collision_loss: 0.34864, value_loss: 0.27951 action_loss: -1.34598\n",
            "update_step:  27 model loss: 1688.05298, kl_loss: 3.00000, obs_loss: 1684.67114, reward_loss: 0.03299, collision_loss: 0.34886, value_loss: 0.27274 action_loss: -1.38285\n",
            "update_step:  28 model loss: 1661.36816, kl_loss: 3.00000, obs_loss: 1657.98535, reward_loss: 0.03397, collision_loss: 0.34886, value_loss: 0.26960 action_loss: -1.43161\n",
            "update_step:  29 model loss: 1648.85437, kl_loss: 3.00000, obs_loss: 1645.47388, reward_loss: 0.03154, collision_loss: 0.34898, value_loss: 0.26999 action_loss: -1.49434\n",
            "update_step:  30 model loss: 1617.20520, kl_loss: 3.00000, obs_loss: 1613.83691, reward_loss: 0.01937, collision_loss: 0.34887, value_loss: 0.26350 action_loss: -1.55747\n",
            "update_step:  31 model loss: 1604.62122, kl_loss: 3.00000, obs_loss: 1601.24438, reward_loss: 0.02767, collision_loss: 0.34917, value_loss: 0.25375 action_loss: -1.61691\n",
            "update_step:  32 model loss: 1595.04492, kl_loss: 3.00000, obs_loss: 1591.66406, reward_loss: 0.03207, collision_loss: 0.34877, value_loss: 0.24149 action_loss: -1.66694\n",
            "update_step:  33 model loss: 1577.64392, kl_loss: 3.00000, obs_loss: 1574.26721, reward_loss: 0.02788, collision_loss: 0.34884, value_loss: 0.23510 action_loss: -1.72603\n",
            "update_step:  34 model loss: 1491.26331, kl_loss: 3.00000, obs_loss: 1487.88977, reward_loss: 0.02487, collision_loss: 0.34867, value_loss: 0.22506 action_loss: -1.78072\n",
            "update_step:  35 model loss: 1571.41150, kl_loss: 3.00000, obs_loss: 1568.03674, reward_loss: 0.02592, collision_loss: 0.34893, value_loss: 0.21455 action_loss: -1.84693\n",
            "update_step:  36 model loss: 1514.25879, kl_loss: 3.00000, obs_loss: 1510.88000, reward_loss: 0.02989, collision_loss: 0.34893, value_loss: 0.21094 action_loss: -1.93468\n",
            "update_step:  37 model loss: 1504.53711, kl_loss: 3.00000, obs_loss: 1501.16907, reward_loss: 0.01893, collision_loss: 0.34914, value_loss: 0.20386 action_loss: -2.02294\n",
            "update_step:  38 model loss: 1463.33923, kl_loss: 3.00000, obs_loss: 1459.95593, reward_loss: 0.03431, collision_loss: 0.34905, value_loss: 0.19528 action_loss: -2.10608\n",
            "update_step:  39 model loss: 1415.80359, kl_loss: 3.00000, obs_loss: 1412.43506, reward_loss: 0.01944, collision_loss: 0.34914, value_loss: 0.19069 action_loss: -2.19509\n",
            "update_step:  40 model loss: 1428.38745, kl_loss: 3.00000, obs_loss: 1425.01733, reward_loss: 0.02097, collision_loss: 0.34917, value_loss: 0.17792 action_loss: -2.26542\n",
            "update_step:  41 model loss: 1390.47766, kl_loss: 3.00000, obs_loss: 1387.09790, reward_loss: 0.03093, collision_loss: 0.34893, value_loss: 0.17489 action_loss: -2.36960\n",
            "update_step:  42 model loss: 1348.18921, kl_loss: 3.00000, obs_loss: 1344.80005, reward_loss: 0.04017, collision_loss: 0.34895, value_loss: 0.17594 action_loss: -2.50225\n",
            "update_step:  43 model loss: 1270.38464, kl_loss: 3.00000, obs_loss: 1267.00623, reward_loss: 0.02985, collision_loss: 0.34860, value_loss: 0.18070 action_loss: -2.65762\n",
            "update_step:  44 model loss: 1312.11755, kl_loss: 3.00000, obs_loss: 1308.74841, reward_loss: 0.02027, collision_loss: 0.34889, value_loss: 0.17670 action_loss: -2.77878\n",
            "update_step:  45 model loss: 1290.08643, kl_loss: 3.00000, obs_loss: 1286.71204, reward_loss: 0.02545, collision_loss: 0.34882, value_loss: 0.17450 action_loss: -2.88957\n",
            "update_step:  46 model loss: 1216.70801, kl_loss: 3.00000, obs_loss: 1213.34412, reward_loss: 0.01498, collision_loss: 0.34889, value_loss: 0.16909 action_loss: -2.98870\n",
            "update_step:  47 model loss: 1228.12720, kl_loss: 3.00000, obs_loss: 1224.75317, reward_loss: 0.02496, collision_loss: 0.34915, value_loss: 0.16986 action_loss: -3.14471\n",
            "update_step:  48 model loss: 1140.27942, kl_loss: 3.00000, obs_loss: 1136.89038, reward_loss: 0.03958, collision_loss: 0.34950, value_loss: 0.17463 action_loss: -3.35061\n",
            "update_step:  49 model loss: 1114.54468, kl_loss: 3.00000, obs_loss: 1111.17566, reward_loss: 0.02002, collision_loss: 0.34894, value_loss: 0.17162 action_loss: -3.54666\n",
            "update_step:  50 model loss: 1137.06189, kl_loss: 3.00000, obs_loss: 1133.68457, reward_loss: 0.02819, collision_loss: 0.34907, value_loss: 0.16808 action_loss: -3.69955\n",
            "elasped time for update: 92.84s\n",
            "episode [   7/ 300] is collected. Total reward is -989.212648\n",
            "elasped time for interaction: 7.29s\n",
            "update_step:   1 model loss: 1041.69629, kl_loss: 3.00000, obs_loss: 1038.31689, reward_loss: 0.03015, collision_loss: 0.34920, value_loss: 0.16794 action_loss: -3.82323\n",
            "update_step:   2 model loss: 1026.65869, kl_loss: 3.00000, obs_loss: 1023.27563, reward_loss: 0.03402, collision_loss: 0.34894, value_loss: 0.16788 action_loss: -3.96742\n",
            "update_step:   3 model loss: 1047.87390, kl_loss: 3.00000, obs_loss: 1044.48767, reward_loss: 0.03682, collision_loss: 0.34940, value_loss: 0.18015 action_loss: -4.19557\n",
            "update_step:   4 model loss: 974.51874, kl_loss: 3.00000, obs_loss: 971.13293, reward_loss: 0.03689, collision_loss: 0.34895, value_loss: 0.19070 action_loss: -4.47598\n",
            "update_step:   5 model loss: 1003.33636, kl_loss: 3.00000, obs_loss: 999.95258, reward_loss: 0.03490, collision_loss: 0.34890, value_loss: 0.19696 action_loss: -4.72718\n",
            "update_step:   6 model loss: 877.89758, kl_loss: 3.00000, obs_loss: 874.53369, reward_loss: 0.01491, collision_loss: 0.34902, value_loss: 0.19947 action_loss: -4.93249\n",
            "update_step:   7 model loss: 898.01147, kl_loss: 3.00000, obs_loss: 894.63696, reward_loss: 0.02535, collision_loss: 0.34916, value_loss: 0.19014 action_loss: -5.09068\n",
            "update_step:   8 model loss: 847.07581, kl_loss: 3.00000, obs_loss: 843.68909, reward_loss: 0.03741, collision_loss: 0.34933, value_loss: 0.19733 action_loss: -5.33752\n",
            "update_step:   9 model loss: 865.96484, kl_loss: 3.00000, obs_loss: 862.58484, reward_loss: 0.03104, collision_loss: 0.34897, value_loss: 0.19672 action_loss: -5.66261\n",
            "update_step:  10 model loss: 842.80383, kl_loss: 3.00000, obs_loss: 839.43372, reward_loss: 0.02113, collision_loss: 0.34898, value_loss: 0.18316 action_loss: -5.94285\n",
            "update_step:  11 model loss: 796.04388, kl_loss: 3.00000, obs_loss: 792.67944, reward_loss: 0.01524, collision_loss: 0.34917, value_loss: 0.16120 action_loss: -6.12489\n",
            "update_step:  12 model loss: 767.04681, kl_loss: 3.00000, obs_loss: 763.66962, reward_loss: 0.02816, collision_loss: 0.34907, value_loss: 0.14498 action_loss: -6.27562\n",
            "update_step:  13 model loss: 751.99634, kl_loss: 3.00000, obs_loss: 748.62878, reward_loss: 0.01854, collision_loss: 0.34899, value_loss: 0.12888 action_loss: -6.49069\n",
            "update_step:  14 model loss: 711.17346, kl_loss: 3.00000, obs_loss: 707.80420, reward_loss: 0.01993, collision_loss: 0.34938, value_loss: 0.11570 action_loss: -6.72008\n",
            "update_step:  15 model loss: 735.85229, kl_loss: 3.00000, obs_loss: 732.48901, reward_loss: 0.01441, collision_loss: 0.34886, value_loss: 0.10564 action_loss: -6.87448\n",
            "update_step:  16 model loss: 737.31445, kl_loss: 3.00000, obs_loss: 733.92102, reward_loss: 0.04431, collision_loss: 0.34915, value_loss: 0.10136 action_loss: -6.98608\n",
            "update_step:  17 model loss: 715.28558, kl_loss: 3.00000, obs_loss: 711.90430, reward_loss: 0.03204, collision_loss: 0.34923, value_loss: 0.10625 action_loss: -7.23132\n",
            "update_step:  18 model loss: 674.87622, kl_loss: 3.00000, obs_loss: 671.50452, reward_loss: 0.02236, collision_loss: 0.34937, value_loss: 0.10552 action_loss: -7.54303\n",
            "update_step:  19 model loss: 655.91144, kl_loss: 3.00000, obs_loss: 652.53491, reward_loss: 0.02774, collision_loss: 0.34881, value_loss: 0.09944 action_loss: -7.75719\n",
            "update_step:  20 model loss: 656.00507, kl_loss: 3.00000, obs_loss: 652.63477, reward_loss: 0.02138, collision_loss: 0.34896, value_loss: 0.09615 action_loss: -7.83212\n",
            "update_step:  21 model loss: 664.71338, kl_loss: 3.00000, obs_loss: 661.34119, reward_loss: 0.02318, collision_loss: 0.34898, value_loss: 0.09319 action_loss: -8.00752\n",
            "update_step:  22 model loss: 630.18756, kl_loss: 3.00000, obs_loss: 626.82532, reward_loss: 0.01282, collision_loss: 0.34942, value_loss: 0.09230 action_loss: -8.24071\n",
            "update_step:  23 model loss: 617.61761, kl_loss: 3.00000, obs_loss: 614.22632, reward_loss: 0.04208, collision_loss: 0.34922, value_loss: 0.08446 action_loss: -8.40297\n",
            "update_step:  24 model loss: 674.67548, kl_loss: 3.00000, obs_loss: 671.29944, reward_loss: 0.02680, collision_loss: 0.34922, value_loss: 0.07999 action_loss: -8.45737\n",
            "update_step:  25 model loss: 627.44440, kl_loss: 3.00000, obs_loss: 624.06775, reward_loss: 0.02729, collision_loss: 0.34934, value_loss: 0.07938 action_loss: -8.60029\n",
            "update_step:  26 model loss: 673.65308, kl_loss: 3.00000, obs_loss: 670.25537, reward_loss: 0.04852, collision_loss: 0.34916, value_loss: 0.08468 action_loss: -8.85805\n",
            "update_step:  27 model loss: 582.20166, kl_loss: 3.00000, obs_loss: 578.82837, reward_loss: 0.02390, collision_loss: 0.34937, value_loss: 0.08631 action_loss: -9.00853\n",
            "update_step:  28 model loss: 637.73792, kl_loss: 3.00000, obs_loss: 634.36951, reward_loss: 0.01876, collision_loss: 0.34966, value_loss: 0.08969 action_loss: -9.07047\n",
            "update_step:  29 model loss: 649.89508, kl_loss: 3.00000, obs_loss: 646.51599, reward_loss: 0.02954, collision_loss: 0.34954, value_loss: 0.09393 action_loss: -9.23716\n",
            "update_step:  30 model loss: 601.65454, kl_loss: 3.00000, obs_loss: 598.28693, reward_loss: 0.01768, collision_loss: 0.34994, value_loss: 0.09605 action_loss: -9.47857\n",
            "update_step:  31 model loss: 571.25348, kl_loss: 3.00000, obs_loss: 567.86963, reward_loss: 0.03444, collision_loss: 0.34940, value_loss: 0.08900 action_loss: -9.64002\n",
            "update_step:  32 model loss: 610.67383, kl_loss: 3.00000, obs_loss: 607.29016, reward_loss: 0.03422, collision_loss: 0.34944, value_loss: 0.09167 action_loss: -9.71555\n",
            "update_step:  33 model loss: 630.83514, kl_loss: 3.00000, obs_loss: 627.45178, reward_loss: 0.03370, collision_loss: 0.34964, value_loss: 0.09457 action_loss: -9.81679\n",
            "update_step:  34 model loss: 599.60663, kl_loss: 3.00000, obs_loss: 596.23297, reward_loss: 0.02402, collision_loss: 0.34962, value_loss: 0.09826 action_loss: -10.01811\n",
            "update_step:  35 model loss: 615.19653, kl_loss: 3.00000, obs_loss: 611.81897, reward_loss: 0.02784, collision_loss: 0.34975, value_loss: 0.09543 action_loss: -10.28273\n",
            "update_step:  36 model loss: 625.86591, kl_loss: 3.00000, obs_loss: 622.49365, reward_loss: 0.02293, collision_loss: 0.34933, value_loss: 0.09035 action_loss: -10.45263\n",
            "update_step:  37 model loss: 682.97876, kl_loss: 3.00000, obs_loss: 679.60638, reward_loss: 0.02261, collision_loss: 0.34981, value_loss: 0.08973 action_loss: -10.54568\n",
            "update_step:  38 model loss: 566.39850, kl_loss: 3.00000, obs_loss: 563.02136, reward_loss: 0.02745, collision_loss: 0.34969, value_loss: 0.09050 action_loss: -10.68066\n",
            "update_step:  39 model loss: 591.20697, kl_loss: 3.00000, obs_loss: 587.83276, reward_loss: 0.02440, collision_loss: 0.34977, value_loss: 0.08881 action_loss: -10.91837\n",
            "update_step:  40 model loss: 560.71564, kl_loss: 3.00000, obs_loss: 557.33569, reward_loss: 0.03000, collision_loss: 0.34989, value_loss: 0.08484 action_loss: -11.20005\n",
            "update_step:  41 model loss: 568.49902, kl_loss: 3.00000, obs_loss: 565.12561, reward_loss: 0.02387, collision_loss: 0.34952, value_loss: 0.07786 action_loss: -11.37446\n",
            "update_step:  42 model loss: 612.30426, kl_loss: 3.00000, obs_loss: 608.93372, reward_loss: 0.02058, collision_loss: 0.34997, value_loss: 0.07971 action_loss: -11.43527\n",
            "update_step:  43 model loss: 602.49658, kl_loss: 3.00000, obs_loss: 599.11267, reward_loss: 0.03419, collision_loss: 0.34972, value_loss: 0.07504 action_loss: -11.39709\n",
            "update_step:  44 model loss: 605.08844, kl_loss: 3.00000, obs_loss: 601.71014, reward_loss: 0.02856, collision_loss: 0.34974, value_loss: 0.07242 action_loss: -11.46156\n",
            "update_step:  45 model loss: 639.25623, kl_loss: 3.00000, obs_loss: 635.89270, reward_loss: 0.01349, collision_loss: 0.35001, value_loss: 0.07327 action_loss: -11.58547\n",
            "update_step:  46 model loss: 600.26324, kl_loss: 3.00000, obs_loss: 596.86707, reward_loss: 0.04648, collision_loss: 0.34968, value_loss: 0.07267 action_loss: -11.76435\n",
            "update_step:  47 model loss: 564.89972, kl_loss: 3.00000, obs_loss: 561.53131, reward_loss: 0.01863, collision_loss: 0.34979, value_loss: 0.06911 action_loss: -11.97338\n",
            "update_step:  48 model loss: 656.67767, kl_loss: 3.00000, obs_loss: 653.30884, reward_loss: 0.01902, collision_loss: 0.34978, value_loss: 0.07054 action_loss: -12.04685\n",
            "update_step:  49 model loss: 765.78046, kl_loss: 3.00000, obs_loss: 762.41699, reward_loss: 0.01401, collision_loss: 0.34947, value_loss: 0.07257 action_loss: -12.01517\n",
            "update_step:  50 model loss: 673.65826, kl_loss: 3.00000, obs_loss: 670.28040, reward_loss: 0.02821, collision_loss: 0.34966, value_loss: 0.07661 action_loss: -11.85009\n",
            "elasped time for update: 92.52s\n",
            "episode [   8/ 300] is collected. Total reward is -1000.000000\n",
            "elasped time for interaction: 2.04s\n",
            "update_step:   1 model loss: 548.31464, kl_loss: 3.00000, obs_loss: 544.94244, reward_loss: 0.02276, collision_loss: 0.34945, value_loss: 0.08088 action_loss: -11.60641\n",
            "update_step:   2 model loss: 575.15625, kl_loss: 3.00000, obs_loss: 571.76892, reward_loss: 0.03784, collision_loss: 0.34951, value_loss: 0.07209 action_loss: -11.49421\n",
            "update_step:   3 model loss: 579.39618, kl_loss: 3.00000, obs_loss: 576.01099, reward_loss: 0.03559, collision_loss: 0.34961, value_loss: 0.06964 action_loss: -11.45864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir='./runs'"
      ],
      "metadata": {
        "id": "J7RQlw4B_NJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}