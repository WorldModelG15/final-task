{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1sYb8Iade6OOsN9KepFlzkkPpEvhooGtH",
      "authorship_tag": "ABX9TyOiSSiBgpVignWCFMwcBOV4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WorldModelG15/final-task/blob/takata/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuQeh7pIyax3",
        "outputId": "8b0a1a3b-fb9c-478f-f7a4-9e435c702d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'final-task' already exists and is not an empty directory.\n",
            "/content/final-task\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive -b takata https://github.com/WorldModelG15/final-task.git\n",
        "%cd final-task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!apt-get install x11-utils -y\n",
        "from pyvirtualdisplay import Display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "def create_display():\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "    if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "        !sh xvfb start\n",
        "        %env DISPLAY=:1\n",
        "create_display()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAMgXohY2P70",
        "outputId": "cb0558ac-98a2-42fb-a60c-b141f47a2834"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gym-duckietown/\n",
        "!pip install -e .\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSCce_MBzmgo",
        "outputId": "18f82fd4-b6f2-4db6-8068-109a67be7edb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/final-task/gym-duckietown\n",
            "Obtaining file:///content/final-task/gym-duckietown\n",
            "Requirement already satisfied: gym>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (1.21.5)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (1.5.0)\n",
            "Requirement already satisfied: pyzmq>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (22.3.0)\n",
            "Requirement already satisfied: opencv-python>=3.4 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (4.1.2.30)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (3.13)\n",
            "Requirement already satisfied: duckietown-world-daffy in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (6.2.38)\n",
            "Requirement already satisfied: PyGeometry-z6 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (2.1.4)\n",
            "Requirement already satisfied: carnivalmirror==0.6.2 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (0.6.2)\n",
            "Requirement already satisfied: zuper-commons-z6 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (6.2.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (3.10.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.31) (7.1.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.31) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.1->duckietown-gym-daffy==6.1.31) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.1->duckietown-gym-daffy==6.1.31) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.31) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.31) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.31) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.31) (3.0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet->duckietown-gym-daffy==6.1.31) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.31) (1.15.0)\n",
            "Requirement already satisfied: gltflib in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0.12)\n",
            "Requirement already satisfied: pyrender in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.1.45)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (2.3.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (5.5.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (5.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (4.2.6)\n",
            "Requirement already satisfied: jpeg4py in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.1.4)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (3.3.6)\n",
            "Requirement already satisfied: oyaml in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0)\n",
            "Requirement already satisfied: aido-protocols-daffy in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (6.0.59)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (3.10.8)\n",
            "Requirement already satisfied: beautifulsoup4<=4.7.1,>=4.6.3 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (4.6.3)\n",
            "Requirement already satisfied: networkx<3,>=2.2 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (2.6.3)\n",
            "Requirement already satisfied: zuper-ipce-z6 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (6.1.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (15.0.1)\n",
            "Requirement already satisfied: duckietown-serialization-ds1<2 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0.20)\n",
            "Requirement already satisfied: PyContracts3 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (3.0.2)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.4.2)\n",
            "Requirement already satisfied: zuper-typing-z6>=6.0.66 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (6.2.3)\n",
            "Requirement already satisfied: base58<2 in /usr/local/lib/python3.7/dist-packages (from duckietown-serialization-ds1<2->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (4.3.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.1.0)\n",
            "Requirement already satisfied: mypy-extensions in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.4.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (2018.9)\n",
            "Requirement already satisfied: pybase64 in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.2.1)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.3.7)\n",
            "Requirement already satisfied: validate-email in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.3)\n",
            "Requirement already satisfied: coverage>=1.4.33 in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (3.7.1)\n",
            "Requirement already satisfied: xtermcolor in /usr/local/lib/python3.7/dist-packages (from zuper-commons-z6->duckietown-gym-daffy==6.1.31) (1.3)\n",
            "Requirement already satisfied: webcolors in /usr/local/lib/python3.7/dist-packages (from zuper-commons-z6->duckietown-gym-daffy==6.1.31) (1.11.1)\n",
            "Requirement already satisfied: zuper-nodes-z6>=6.2.8 in /usr/local/lib/python3.7/dist-packages (from aido-protocols-daffy->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (6.2.13)\n",
            "Requirement already satisfied: cbor2 in /usr/local/lib/python3.7/dist-packages (from zuper-nodes-z6>=6.2.8->aido-protocols-daffy->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (5.4.2.post1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.7/dist-packages (from coloredlogs->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (10.0)\n",
            "Requirement already satisfied: dataclasses-json>=0.4.5 in /usr/local/lib/python3.7/dist-packages (from gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.5.7)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from dataclasses-json>=0.4.5->gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.5.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from dataclasses-json>=0.4.5->gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (3.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from dataclasses-json>=0.4.5->gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json>=0.4.5->gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (21.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.2.5)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from jpeg4py->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->jpeg4py->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (2.21)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (4.11.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (5.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (3.7.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.7.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (8.0.1)\n",
            "Requirement already satisfied: PyOpenGL==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pyrender->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (3.1.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from pyrender->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (2.4.1)\n",
            "Requirement already satisfied: freetype-py in /usr/local/lib/python3.7/dist-packages (from pyrender->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (2.2.0)\n",
            "Requirement already satisfied: py-multihash in /usr/local/lib/python3.7/dist-packages (from zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.2.3)\n",
            "Requirement already satisfied: py-cid in /usr/local/lib/python3.7/dist-packages (from zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.3.0)\n",
            "Requirement already satisfied: morphys<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0)\n",
            "Requirement already satisfied: py-multibase<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0.3)\n",
            "Requirement already satisfied: py-multicodec<0.3.0 in /usr/local/lib/python3.7/dist-packages (from py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (0.2.1)\n",
            "Requirement already satisfied: python-baseconv<2.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from py-multibase<2.0.0,>=1.0.0->py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.2.2)\n",
            "Requirement already satisfied: varint<2.0.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from py-multicodec<0.3.0->py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.31) (1.0.2)\n",
            "Installing collected packages: duckietown-gym-daffy\n",
            "  Attempting uninstall: duckietown-gym-daffy\n",
            "    Found existing installation: duckietown-gym-daffy 6.1.31\n",
            "    Can't uninstall 'duckietown-gym-daffy'. No files were found to uninstall.\n",
            "  Running setup.py develop for duckietown-gym-daffy\n",
            "Successfully installed duckietown-gym-daffy-6.1.31\n",
            "/content/final-task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd final-task"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW8EaSH261dx",
        "outputId": "f7806732-ee99-437a-aa87-04dcf58b01b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'final-task'\n",
            "/content/final-task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/final-task/gym-duckietown/src/')\n",
        "from utils.env import launch_env\n",
        "from utils.wrappers import (\n",
        "    EarlyStopWrapper,\n",
        "    NormalizeWrapper,\n",
        "    ImgWrapper,\n",
        "    DtRewardWrapper,\n",
        "    ActionWrapper,\n",
        "    ResizeWrapper,\n",
        "    EarlyStopWrapper,\n",
        ")\n",
        "from dreamer.config import DreamerConfig\n",
        "from dreamer.trainer import Trainer\n",
        "import torch\n",
        "\n",
        "env = launch_env(map_name=\"loop_pedestrians\")\n",
        "env = ResizeWrapper(env)\n",
        "# env = NormalizeWrapper(env)\n",
        "env = ImgWrapper(env)  # to make the images from 120x160x3 into 3x120x160\n",
        "env = ActionWrapper(env)\n",
        "env = DtRewardWrapper(env)\n",
        "env = EarlyStopWrapper(env)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "config = DreamerConfig()\n",
        "trainer = Trainer(env, device, config)\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yXzAfRTIFQo8",
        "outputId": "2121d645-5c10-4938-c9a8-7a7b6fc654b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:commons:version: 6.2.4 *\n",
            "DEBUG:typing:version: 6.2.3\n",
            "DEBUG:duckietown_world:duckietown-world version 6.2.38 path /usr/local/lib/python3.7/dist-packages\n",
            "DEBUG:geometry:PyGeometry-z6 version 2.1.4 path /usr/local/lib/python3.7/dist-packages\n",
            "DEBUG:aido_schemas:aido-protocols version 6.0.59 path /usr/local/lib/python3.7/dist-packages\n",
            "DEBUG:nodes:version 6.2.13 path /usr/local/lib/python3.7/dist-packages pyparsing 3.0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': ('directsound', 'openal', 'pulse', 'silent'), 'debug_font': False, 'debug_gl': True, 'debug_gl_trace': False, 'debug_gl_trace_args': False, 'debug_graphics_batch': False, 'debug_lib': False, 'debug_media': False, 'debug_texture': False, 'debug_trace': False, 'debug_trace_args': False, 'debug_trace_depth': 1, 'debug_trace_flush': True, 'debug_win32': False, 'debug_x11': False, 'graphics_vbo': True, 'shadow_window': True, 'vsync': None, 'xsync': True, 'xlib_fullscreen_override_redirect': False, 'darwin_cocoa': True, 'search_local_libs': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:duckietown_world: data: /usr/local/lib/python3.7/dist-packages/duckietown_world/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode [   6/ 300] is collected. Total reward is -991.560542\n",
            "elasped time for interaction: 17.25s\n",
            "update_step:   1 model loss: 3570.00244, kl_loss: 3.00000, obs_loss: 3566.97559, reward_loss: 0.02677, value_loss: 0.06055 action_loss: 0.22380\n",
            "update_step:   2 model loss: 3447.91089, kl_loss: 3.00000, obs_loss: 3444.87769, reward_loss: 0.03315, value_loss: 0.03815 action_loss: 0.17606\n",
            "update_step:   3 model loss: 3425.03223, kl_loss: 3.00000, obs_loss: 3422.01465, reward_loss: 0.01765, value_loss: 0.02252 action_loss: 0.13506\n",
            "update_step:   4 model loss: 3496.52441, kl_loss: 3.00000, obs_loss: 3493.48608, reward_loss: 0.03841, value_loss: 0.01108 action_loss: 0.09041\n",
            "update_step:   5 model loss: 3439.51538, kl_loss: 3.00000, obs_loss: 3436.48975, reward_loss: 0.02556, value_loss: 0.00469 action_loss: 0.05060\n",
            "update_step:   6 model loss: 3544.37476, kl_loss: 3.00000, obs_loss: 3541.33350, reward_loss: 0.04129, value_loss: 0.00245 action_loss: 0.00453\n",
            "update_step:   7 model loss: 3495.64673, kl_loss: 3.00000, obs_loss: 3492.60156, reward_loss: 0.04518, value_loss: 0.00467 action_loss: -0.04218\n",
            "update_step:   8 model loss: 3314.54150, kl_loss: 3.00000, obs_loss: 3311.52734, reward_loss: 0.01416, value_loss: 0.01116 action_loss: -0.08790\n",
            "update_step:   9 model loss: 3324.70825, kl_loss: 3.00000, obs_loss: 3321.67627, reward_loss: 0.03199, value_loss: 0.02207 action_loss: -0.13855\n",
            "update_step:  10 model loss: 3270.29370, kl_loss: 3.00000, obs_loss: 3267.27563, reward_loss: 0.01819, value_loss: 0.03657 action_loss: -0.18913\n",
            "update_step:  11 model loss: 3062.54321, kl_loss: 3.00000, obs_loss: 3059.52441, reward_loss: 0.01889, value_loss: 0.05498 action_loss: -0.24370\n",
            "update_step:  12 model loss: 3004.07690, kl_loss: 3.00000, obs_loss: 3001.06079, reward_loss: 0.01623, value_loss: 0.07751 action_loss: -0.30251\n",
            "update_step:  13 model loss: 2985.89355, kl_loss: 3.00000, obs_loss: 2982.87695, reward_loss: 0.01669, value_loss: 0.09771 action_loss: -0.35780\n",
            "update_step:  14 model loss: 2745.61548, kl_loss: 3.00000, obs_loss: 2742.60010, reward_loss: 0.01546, value_loss: 0.11489 action_loss: -0.40818\n",
            "update_step:  15 model loss: 2637.22656, kl_loss: 3.00000, obs_loss: 2634.19312, reward_loss: 0.03339, value_loss: 0.13237 action_loss: -0.45725\n",
            "update_step:  16 model loss: 2467.70386, kl_loss: 3.00000, obs_loss: 2464.69678, reward_loss: 0.00715, value_loss: 0.14936 action_loss: -0.50399\n",
            "update_step:  17 model loss: 2386.37109, kl_loss: 3.00000, obs_loss: 2383.36304, reward_loss: 0.00795, value_loss: 0.16670 action_loss: -0.55068\n",
            "update_step:  18 model loss: 2271.27539, kl_loss: 3.00000, obs_loss: 2268.26587, reward_loss: 0.00944, value_loss: 0.17985 action_loss: -0.59072\n",
            "update_step:  19 model loss: 2223.98877, kl_loss: 3.00000, obs_loss: 2220.96973, reward_loss: 0.01906, value_loss: 0.18804 action_loss: -0.62516\n",
            "update_step:  20 model loss: 2108.28271, kl_loss: 3.00000, obs_loss: 2105.26270, reward_loss: 0.01992, value_loss: 0.19232 action_loss: -0.65246\n",
            "update_step:  21 model loss: 1982.57983, kl_loss: 3.00000, obs_loss: 1979.56689, reward_loss: 0.01295, value_loss: 0.19550 action_loss: -0.67835\n",
            "update_step:  22 model loss: 1923.52185, kl_loss: 3.00000, obs_loss: 1920.51489, reward_loss: 0.00696, value_loss: 0.19211 action_loss: -0.69593\n",
            "update_step:  23 model loss: 1865.89392, kl_loss: 3.00000, obs_loss: 1862.88208, reward_loss: 0.01181, value_loss: 0.18782 action_loss: -0.71429\n",
            "update_step:  24 model loss: 1768.51636, kl_loss: 3.00000, obs_loss: 1765.51050, reward_loss: 0.00584, value_loss: 0.17727 action_loss: -0.72398\n",
            "update_step:  25 model loss: 1737.78564, kl_loss: 3.00000, obs_loss: 1734.75269, reward_loss: 0.03296, value_loss: 0.16729 action_loss: -0.73262\n",
            "update_step:  26 model loss: 1689.18176, kl_loss: 3.00000, obs_loss: 1686.15576, reward_loss: 0.02596, value_loss: 0.16348 action_loss: -0.75226\n",
            "update_step:  27 model loss: 1644.87708, kl_loss: 3.00000, obs_loss: 1641.85571, reward_loss: 0.02130, value_loss: 0.16149 action_loss: -0.77522\n",
            "update_step:  28 model loss: 1564.88562, kl_loss: 3.00000, obs_loss: 1561.86255, reward_loss: 0.02310, value_loss: 0.16055 action_loss: -0.80256\n",
            "update_step:  29 model loss: 1500.00842, kl_loss: 3.00000, obs_loss: 1496.99219, reward_loss: 0.01623, value_loss: 0.16002 action_loss: -0.83278\n",
            "update_step:  30 model loss: 1532.94336, kl_loss: 3.00000, obs_loss: 1529.91541, reward_loss: 0.02792, value_loss: 0.15768 action_loss: -0.85536\n",
            "update_step:  31 model loss: 1480.73560, kl_loss: 3.00000, obs_loss: 1477.71240, reward_loss: 0.02322, value_loss: 0.15689 action_loss: -0.88258\n",
            "update_step:  32 model loss: 1422.87927, kl_loss: 3.00000, obs_loss: 1419.85559, reward_loss: 0.02364, value_loss: 0.15747 action_loss: -0.91844\n",
            "update_step:  33 model loss: 1457.98755, kl_loss: 3.00000, obs_loss: 1454.95886, reward_loss: 0.02866, value_loss: 0.15709 action_loss: -0.95070\n",
            "update_step:  34 model loss: 1438.54419, kl_loss: 3.00000, obs_loss: 1435.52588, reward_loss: 0.01828, value_loss: 0.15624 action_loss: -0.97934\n",
            "update_step:  35 model loss: 1434.03857, kl_loss: 3.00000, obs_loss: 1431.02344, reward_loss: 0.01514, value_loss: 0.15626 action_loss: -1.01388\n",
            "update_step:  36 model loss: 1389.29529, kl_loss: 3.00000, obs_loss: 1386.28467, reward_loss: 0.01062, value_loss: 0.15344 action_loss: -1.04882\n",
            "update_step:  37 model loss: 1363.00989, kl_loss: 3.00000, obs_loss: 1359.98083, reward_loss: 0.02908, value_loss: 0.14933 action_loss: -1.07994\n",
            "update_step:  38 model loss: 1354.69592, kl_loss: 3.00000, obs_loss: 1351.67444, reward_loss: 0.02143, value_loss: 0.14445 action_loss: -1.10613\n",
            "update_step:  39 model loss: 1348.45117, kl_loss: 3.00000, obs_loss: 1345.42627, reward_loss: 0.02492, value_loss: 0.14483 action_loss: -1.14700\n",
            "update_step:  40 model loss: 1305.35205, kl_loss: 3.00000, obs_loss: 1302.33765, reward_loss: 0.01436, value_loss: 0.14243 action_loss: -1.18644\n",
            "update_step:  41 model loss: 1313.71484, kl_loss: 3.00000, obs_loss: 1310.68652, reward_loss: 0.02836, value_loss: 0.14369 action_loss: -1.23577\n",
            "update_step:  42 model loss: 1333.63525, kl_loss: 3.00000, obs_loss: 1330.62061, reward_loss: 0.01460, value_loss: 0.13757 action_loss: -1.26498\n",
            "update_step:  43 model loss: 1308.12146, kl_loss: 3.00000, obs_loss: 1305.10095, reward_loss: 0.02046, value_loss: 0.13743 action_loss: -1.31073\n",
            "update_step:  44 model loss: 1278.68066, kl_loss: 3.00000, obs_loss: 1275.65234, reward_loss: 0.02833, value_loss: 0.13820 action_loss: -1.37484\n",
            "update_step:  45 model loss: 1273.55212, kl_loss: 3.00000, obs_loss: 1270.52307, reward_loss: 0.02903, value_loss: 0.13678 action_loss: -1.42870\n",
            "update_step:  46 model loss: 1237.57642, kl_loss: 3.00000, obs_loss: 1234.55286, reward_loss: 0.02351, value_loss: 0.13724 action_loss: -1.48460\n",
            "update_step:  47 model loss: 1256.50476, kl_loss: 3.00000, obs_loss: 1253.48181, reward_loss: 0.02300, value_loss: 0.14096 action_loss: -1.55432\n",
            "update_step:  48 model loss: 1245.61768, kl_loss: 3.00000, obs_loss: 1242.58691, reward_loss: 0.03075, value_loss: 0.14799 action_loss: -1.64101\n",
            "update_step:  49 model loss: 1230.28345, kl_loss: 3.00000, obs_loss: 1227.25024, reward_loss: 0.03316, value_loss: 0.15574 action_loss: -1.73442\n",
            "update_step:  50 model loss: 1254.97888, kl_loss: 3.00000, obs_loss: 1251.96082, reward_loss: 0.01811, value_loss: 0.16149 action_loss: -1.81763\n",
            "elasped time for update: 72.85s\n",
            "episode [   7/ 300] is collected. Total reward is -759.116730\n",
            "elasped time for interaction: 85.91s\n",
            "update_step:   1 model loss: 1218.58850, kl_loss: 3.00000, obs_loss: 1215.57373, reward_loss: 0.01475, value_loss: 0.16126 action_loss: -1.88618\n",
            "update_step:   2 model loss: 1185.67944, kl_loss: 3.00000, obs_loss: 1182.66724, reward_loss: 0.01220, value_loss: 0.16040 action_loss: -1.97432\n",
            "update_step:   3 model loss: 1183.92493, kl_loss: 3.00000, obs_loss: 1180.89648, reward_loss: 0.02841, value_loss: 0.15816 action_loss: -2.08161\n",
            "update_step:   4 model loss: 1226.49915, kl_loss: 3.00000, obs_loss: 1223.48560, reward_loss: 0.01353, value_loss: 0.15186 action_loss: -2.16726\n",
            "update_step:   5 model loss: 1178.20117, kl_loss: 3.00000, obs_loss: 1175.15479, reward_loss: 0.04633, value_loss: 0.14728 action_loss: -2.23729\n",
            "update_step:   6 model loss: 1218.43213, kl_loss: 3.00000, obs_loss: 1215.38989, reward_loss: 0.04228, value_loss: 0.15220 action_loss: -2.33187\n",
            "update_step:   7 model loss: 1174.26270, kl_loss: 3.00000, obs_loss: 1171.22900, reward_loss: 0.03368, value_loss: 0.15817 action_loss: -2.46664\n",
            "update_step:   8 model loss: 1137.05957, kl_loss: 3.00000, obs_loss: 1134.03833, reward_loss: 0.02118, value_loss: 0.16388 action_loss: -2.61879\n",
            "update_step:   9 model loss: 1233.81335, kl_loss: 3.00000, obs_loss: 1230.79456, reward_loss: 0.01883, value_loss: 0.16325 action_loss: -2.73621\n",
            "update_step:  10 model loss: 1130.13525, kl_loss: 3.00000, obs_loss: 1127.10669, reward_loss: 0.02852, value_loss: 0.16236 action_loss: -2.82421\n",
            "update_step:  11 model loss: 1123.28699, kl_loss: 3.00000, obs_loss: 1120.26392, reward_loss: 0.02311, value_loss: 0.16131 action_loss: -2.92252\n",
            "update_step:  12 model loss: 1152.80212, kl_loss: 3.00000, obs_loss: 1149.77942, reward_loss: 0.02274, value_loss: 0.16322 action_loss: -3.07238\n",
            "update_step:  13 model loss: 1140.97485, kl_loss: 3.00000, obs_loss: 1137.95129, reward_loss: 0.02359, value_loss: 0.16091 action_loss: -3.23521\n",
            "update_step:  14 model loss: 1133.96436, kl_loss: 3.00000, obs_loss: 1130.94128, reward_loss: 0.02305, value_loss: 0.15520 action_loss: -3.37754\n",
            "update_step:  15 model loss: 1138.34998, kl_loss: 3.00000, obs_loss: 1135.33179, reward_loss: 0.01814, value_loss: 0.15251 action_loss: -3.50177\n",
            "update_step:  16 model loss: 1108.13550, kl_loss: 3.00000, obs_loss: 1105.12451, reward_loss: 0.01104, value_loss: 0.14441 action_loss: -3.61428\n",
            "update_step:  17 model loss: 1104.31250, kl_loss: 3.00000, obs_loss: 1101.27661, reward_loss: 0.03590, value_loss: 0.13975 action_loss: -3.78161\n",
            "update_step:  18 model loss: 1076.37683, kl_loss: 3.00000, obs_loss: 1073.35352, reward_loss: 0.02326, value_loss: 0.14121 action_loss: -3.98413\n",
            "update_step:  19 model loss: 1087.67822, kl_loss: 3.00000, obs_loss: 1084.66772, reward_loss: 0.01047, value_loss: 0.13084 action_loss: -4.13817\n",
            "update_step:  20 model loss: 1096.93054, kl_loss: 3.00000, obs_loss: 1093.90552, reward_loss: 0.02504, value_loss: 0.12438 action_loss: -4.28005\n",
            "update_step:  21 model loss: 1061.92444, kl_loss: 3.00000, obs_loss: 1058.91077, reward_loss: 0.01366, value_loss: 0.11869 action_loss: -4.40144\n",
            "update_step:  22 model loss: 1093.56812, kl_loss: 3.00000, obs_loss: 1090.54395, reward_loss: 0.02416, value_loss: 0.11315 action_loss: -4.57718\n",
            "update_step:  23 model loss: 1056.89172, kl_loss: 3.00000, obs_loss: 1053.85706, reward_loss: 0.03465, value_loss: 0.11296 action_loss: -4.81310\n",
            "update_step:  24 model loss: 1080.38184, kl_loss: 3.00000, obs_loss: 1077.35498, reward_loss: 0.02688, value_loss: 0.10799 action_loss: -5.00722\n",
            "update_step:  25 model loss: 1018.15338, kl_loss: 3.00000, obs_loss: 1015.11749, reward_loss: 0.03587, value_loss: 0.10464 action_loss: -5.15556\n",
            "update_step:  26 model loss: 1012.48444, kl_loss: 3.00000, obs_loss: 1009.46411, reward_loss: 0.02034, value_loss: 0.10146 action_loss: -5.29844\n",
            "update_step:  27 model loss: 1057.20093, kl_loss: 3.00000, obs_loss: 1054.17578, reward_loss: 0.02510, value_loss: 0.10079 action_loss: -5.49785\n",
            "update_step:  28 model loss: 998.31329, kl_loss: 3.00000, obs_loss: 995.26917, reward_loss: 0.04410, value_loss: 0.09777 action_loss: -5.74841\n",
            "update_step:  29 model loss: 1042.44189, kl_loss: 3.00000, obs_loss: 1039.40332, reward_loss: 0.03860, value_loss: 0.09437 action_loss: -5.96265\n",
            "update_step:  30 model loss: 1029.85144, kl_loss: 3.00000, obs_loss: 1026.83069, reward_loss: 0.02072, value_loss: 0.08992 action_loss: -6.13450\n",
            "update_step:  31 model loss: 999.91217, kl_loss: 3.00000, obs_loss: 996.89270, reward_loss: 0.01945, value_loss: 0.08650 action_loss: -6.32789\n",
            "update_step:  32 model loss: 1027.10059, kl_loss: 3.00000, obs_loss: 1024.07617, reward_loss: 0.02437, value_loss: 0.08459 action_loss: -6.55517\n",
            "update_step:  33 model loss: 966.82410, kl_loss: 3.00000, obs_loss: 963.80823, reward_loss: 0.01588, value_loss: 0.07640 action_loss: -6.79692\n",
            "update_step:  34 model loss: 998.89197, kl_loss: 3.00000, obs_loss: 995.86847, reward_loss: 0.02352, value_loss: 0.07068 action_loss: -6.99649\n",
            "update_step:  35 model loss: 963.53204, kl_loss: 3.00000, obs_loss: 960.51514, reward_loss: 0.01689, value_loss: 0.06490 action_loss: -7.18620\n",
            "update_step:  36 model loss: 968.31012, kl_loss: 3.00000, obs_loss: 965.28741, reward_loss: 0.02271, value_loss: 0.05981 action_loss: -7.39115\n",
            "update_step:  37 model loss: 951.90356, kl_loss: 3.00000, obs_loss: 948.88043, reward_loss: 0.02314, value_loss: 0.05541 action_loss: -7.61323\n",
            "update_step:  38 model loss: 902.05719, kl_loss: 3.00000, obs_loss: 899.04352, reward_loss: 0.01370, value_loss: 0.04979 action_loss: -7.81672\n",
            "update_step:  39 model loss: 896.72253, kl_loss: 3.00000, obs_loss: 893.70581, reward_loss: 0.01672, value_loss: 0.04633 action_loss: -7.98217\n",
            "update_step:  40 model loss: 925.48236, kl_loss: 3.00000, obs_loss: 922.45898, reward_loss: 0.02337, value_loss: 0.04697 action_loss: -8.10730\n",
            "update_step:  41 model loss: 882.11322, kl_loss: 3.00000, obs_loss: 879.07788, reward_loss: 0.03532, value_loss: 0.04630 action_loss: -8.27811\n",
            "update_step:  42 model loss: 893.92206, kl_loss: 3.00000, obs_loss: 890.87427, reward_loss: 0.04778, value_loss: 0.04759 action_loss: -8.45281\n",
            "update_step:  43 model loss: 893.11731, kl_loss: 3.00000, obs_loss: 890.09424, reward_loss: 0.02309, value_loss: 0.04606 action_loss: -8.61835\n",
            "update_step:  44 model loss: 861.14648, kl_loss: 3.00000, obs_loss: 858.12683, reward_loss: 0.01967, value_loss: 0.04471 action_loss: -8.71243\n",
            "update_step:  45 model loss: 824.14563, kl_loss: 3.00000, obs_loss: 821.12732, reward_loss: 0.01833, value_loss: 0.04477 action_loss: -8.90270\n",
            "update_step:  46 model loss: 851.53577, kl_loss: 3.00000, obs_loss: 848.52222, reward_loss: 0.01356, value_loss: 0.04377 action_loss: -9.01770\n",
            "update_step:  47 model loss: 810.09808, kl_loss: 3.00000, obs_loss: 807.05524, reward_loss: 0.04285, value_loss: 0.04477 action_loss: -9.15392\n",
            "update_step:  48 model loss: 797.16016, kl_loss: 3.00000, obs_loss: 794.14691, reward_loss: 0.01327, value_loss: 0.04518 action_loss: -9.27802\n",
            "update_step:  49 model loss: 748.35254, kl_loss: 3.00000, obs_loss: 745.33441, reward_loss: 0.01811, value_loss: 0.04431 action_loss: -9.42575\n",
            "update_step:  50 model loss: 792.60852, kl_loss: 3.00000, obs_loss: 789.58795, reward_loss: 0.02056, value_loss: 0.04419 action_loss: -9.48124\n",
            "elasped time for update: 72.61s\n",
            "episode [   8/ 300] is collected. Total reward is -895.138945\n",
            "elasped time for interaction: 53.52s\n",
            "update_step:   1 model loss: 711.45746, kl_loss: 3.00000, obs_loss: 708.43115, reward_loss: 0.02632, value_loss: 0.04393 action_loss: -9.59055\n",
            "update_step:   2 model loss: 728.36475, kl_loss: 3.00000, obs_loss: 725.34424, reward_loss: 0.02049, value_loss: 0.04488 action_loss: -9.64781\n",
            "update_step:   3 model loss: 725.53210, kl_loss: 3.00000, obs_loss: 722.51514, reward_loss: 0.01694, value_loss: 0.04122 action_loss: -9.70900\n",
            "update_step:   4 model loss: 735.32794, kl_loss: 3.00000, obs_loss: 732.29163, reward_loss: 0.03632, value_loss: 0.04235 action_loss: -9.72566\n",
            "update_step:   5 model loss: 662.69543, kl_loss: 3.00000, obs_loss: 659.68494, reward_loss: 0.01049, value_loss: 0.04363 action_loss: -9.79041\n",
            "update_step:   6 model loss: 700.01270, kl_loss: 3.00000, obs_loss: 696.99048, reward_loss: 0.02223, value_loss: 0.04514 action_loss: -9.70752\n",
            "update_step:   7 model loss: 651.20209, kl_loss: 3.00000, obs_loss: 648.17120, reward_loss: 0.03085, value_loss: 0.04053 action_loss: -9.64145\n",
            "update_step:   8 model loss: 647.81342, kl_loss: 3.00000, obs_loss: 644.77393, reward_loss: 0.03949, value_loss: 0.03598 action_loss: -9.54360\n",
            "update_step:   9 model loss: 663.60577, kl_loss: 3.00000, obs_loss: 660.56470, reward_loss: 0.04107, value_loss: 0.03386 action_loss: -9.52835\n",
            "update_step:  10 model loss: 659.44312, kl_loss: 3.00000, obs_loss: 656.43604, reward_loss: 0.00708, value_loss: 0.03704 action_loss: -9.38692\n",
            "update_step:  11 model loss: 629.28619, kl_loss: 3.00000, obs_loss: 626.27026, reward_loss: 0.01592, value_loss: 0.03426 action_loss: -9.35030\n",
            "update_step:  12 model loss: 629.63837, kl_loss: 3.00000, obs_loss: 626.60425, reward_loss: 0.03413, value_loss: 0.02976 action_loss: -9.22843\n",
            "update_step:  13 model loss: 633.40277, kl_loss: 3.00000, obs_loss: 630.38361, reward_loss: 0.01914, value_loss: 0.02621 action_loss: -9.21485\n",
            "update_step:  14 model loss: 631.62988, kl_loss: 3.00000, obs_loss: 628.61737, reward_loss: 0.01251, value_loss: 0.02458 action_loss: -9.07802\n",
            "update_step:  15 model loss: 630.81122, kl_loss: 3.00000, obs_loss: 627.79407, reward_loss: 0.01714, value_loss: 0.02330 action_loss: -8.97046\n",
            "update_step:  16 model loss: 622.36389, kl_loss: 3.00000, obs_loss: 619.33008, reward_loss: 0.03379, value_loss: 0.02337 action_loss: -8.95950\n",
            "update_step:  17 model loss: 612.95422, kl_loss: 3.00000, obs_loss: 609.93933, reward_loss: 0.01489, value_loss: 0.02610 action_loss: -8.82777\n",
            "update_step:  18 model loss: 628.19775, kl_loss: 3.00000, obs_loss: 625.17566, reward_loss: 0.02209, value_loss: 0.02758 action_loss: -8.59934\n",
            "update_step:  19 model loss: 616.86334, kl_loss: 3.00000, obs_loss: 613.84076, reward_loss: 0.02259, value_loss: 0.02601 action_loss: -8.48672\n",
            "update_step:  20 model loss: 621.03217, kl_loss: 3.00000, obs_loss: 618.00299, reward_loss: 0.02917, value_loss: 0.02529 action_loss: -8.47130\n",
            "update_step:  21 model loss: 632.44507, kl_loss: 3.00000, obs_loss: 629.41803, reward_loss: 0.02704, value_loss: 0.02682 action_loss: -8.36933\n",
            "update_step:  22 model loss: 553.91254, kl_loss: 3.00000, obs_loss: 550.89746, reward_loss: 0.01511, value_loss: 0.02898 action_loss: -8.18390\n",
            "update_step:  23 model loss: 556.06427, kl_loss: 3.00000, obs_loss: 553.03760, reward_loss: 0.02669, value_loss: 0.02693 action_loss: -8.09384\n",
            "update_step:  24 model loss: 583.79785, kl_loss: 3.00000, obs_loss: 580.77917, reward_loss: 0.01866, value_loss: 0.02484 action_loss: -8.03248\n",
            "update_step:  25 model loss: 578.45697, kl_loss: 3.30603, obs_loss: 575.13049, reward_loss: 0.02047, value_loss: 0.02734 action_loss: -7.87047\n",
            "update_step:  26 model loss: 552.09143, kl_loss: 3.06742, obs_loss: 548.99622, reward_loss: 0.02774, value_loss: 0.02676 action_loss: -7.63003\n",
            "update_step:  27 model loss: 565.78473, kl_loss: 3.00000, obs_loss: 562.77588, reward_loss: 0.00887, value_loss: 0.02068 action_loss: -7.48648\n",
            "update_step:  28 model loss: 563.84174, kl_loss: 3.00900, obs_loss: 560.82043, reward_loss: 0.01233, value_loss: 0.01803 action_loss: -7.43623\n",
            "update_step:  29 model loss: 584.20587, kl_loss: 3.43623, obs_loss: 580.75635, reward_loss: 0.01333, value_loss: 0.01728 action_loss: -7.31534\n",
            "update_step:  30 model loss: 570.28290, kl_loss: 3.27687, obs_loss: 566.97180, reward_loss: 0.03422, value_loss: 0.01699 action_loss: -7.13608\n",
            "update_step:  31 model loss: 553.70166, kl_loss: 3.00000, obs_loss: 550.68359, reward_loss: 0.01806, value_loss: 0.01298 action_loss: -7.06385\n",
            "update_step:  32 model loss: 534.12921, kl_loss: 3.00417, obs_loss: 531.11432, reward_loss: 0.01075, value_loss: 0.01176 action_loss: -7.06917\n",
            "update_step:  33 model loss: 592.92670, kl_loss: 3.93389, obs_loss: 588.98267, reward_loss: 0.01014, value_loss: 0.01245 action_loss: -6.98819\n",
            "update_step:  34 model loss: 548.76849, kl_loss: 3.58359, obs_loss: 545.15833, reward_loss: 0.02658, value_loss: 0.01413 action_loss: -6.82397\n",
            "update_step:  35 model loss: 578.37164, kl_loss: 4.69484, obs_loss: 573.64441, reward_loss: 0.03241, value_loss: 0.01968 action_loss: -6.69857\n",
            "update_step:  36 model loss: 566.04132, kl_loss: 3.70513, obs_loss: 562.30994, reward_loss: 0.02627, value_loss: 0.01396 action_loss: -6.67770\n",
            "update_step:  37 model loss: 519.05389, kl_loss: 3.00912, obs_loss: 516.02075, reward_loss: 0.02406, value_loss: 0.01533 action_loss: -6.69133\n",
            "update_step:  38 model loss: 543.80475, kl_loss: 3.00620, obs_loss: 540.78033, reward_loss: 0.01820, value_loss: 0.02589 action_loss: -6.65633\n",
            "update_step:  39 model loss: 554.95007, kl_loss: 3.02922, obs_loss: 551.89917, reward_loss: 0.02165, value_loss: 0.04229 action_loss: -6.69673\n",
            "update_step:  40 model loss: 539.73950, kl_loss: 3.00008, obs_loss: 536.70593, reward_loss: 0.03349, value_loss: 0.04534 action_loss: -6.73685\n",
            "update_step:  41 model loss: 520.20337, kl_loss: 3.00000, obs_loss: 517.19073, reward_loss: 0.01264, value_loss: 0.03691 action_loss: -6.81229\n",
            "update_step:  42 model loss: 544.11609, kl_loss: 3.03877, obs_loss: 541.04486, reward_loss: 0.03246, value_loss: 0.03052 action_loss: -6.86714\n",
            "update_step:  43 model loss: 510.94296, kl_loss: 3.72920, obs_loss: 507.16815, reward_loss: 0.04562, value_loss: 0.03231 action_loss: -7.00067\n",
            "update_step:  44 model loss: 521.43848, kl_loss: 4.58682, obs_loss: 516.84161, reward_loss: 0.01004, value_loss: 0.02849 action_loss: -7.05201\n",
            "update_step:  45 model loss: 484.17917, kl_loss: 3.54797, obs_loss: 480.60077, reward_loss: 0.03044, value_loss: 0.02854 action_loss: -7.18223\n",
            "update_step:  46 model loss: 457.88702, kl_loss: 3.28082, obs_loss: 454.57217, reward_loss: 0.03404, value_loss: 0.03814 action_loss: -7.38289\n",
            "update_step:  47 model loss: 537.82928, kl_loss: 3.09925, obs_loss: 534.69672, reward_loss: 0.03330, value_loss: 0.05189 action_loss: -7.54065\n",
            "update_step:  48 model loss: 543.51575, kl_loss: 3.13965, obs_loss: 540.35724, reward_loss: 0.01886, value_loss: 0.05835 action_loss: -7.64744\n",
            "update_step:  49 model loss: 534.29932, kl_loss: 3.13003, obs_loss: 531.12323, reward_loss: 0.04606, value_loss: 0.07483 action_loss: -7.87206\n",
            "update_step:  50 model loss: 529.28107, kl_loss: 3.12936, obs_loss: 526.13379, reward_loss: 0.01792, value_loss: 0.10193 action_loss: -8.25495\n",
            "elasped time for update: 72.99s\n",
            "episode [   9/ 300] is collected. Total reward is -988.209621\n",
            "elasped time for interaction: 5.67s\n",
            "update_step:   1 model loss: 500.81845, kl_loss: 3.24644, obs_loss: 497.55435, reward_loss: 0.01767, value_loss: 0.12836 action_loss: -8.89153\n",
            "update_step:   2 model loss: 528.72699, kl_loss: 3.25311, obs_loss: 525.45648, reward_loss: 0.01737, value_loss: 0.14542 action_loss: -9.49211\n",
            "update_step:   3 model loss: 508.21896, kl_loss: 3.19036, obs_loss: 505.01624, reward_loss: 0.01236, value_loss: 0.15350 action_loss: -9.98585\n",
            "update_step:   4 model loss: 555.12012, kl_loss: 3.24458, obs_loss: 551.83679, reward_loss: 0.03878, value_loss: 0.21338 action_loss: -10.47342\n",
            "update_step:   5 model loss: 548.12421, kl_loss: 3.35461, obs_loss: 544.75598, reward_loss: 0.01363, value_loss: 0.24967 action_loss: -10.96480\n",
            "update_step:   6 model loss: 482.48688, kl_loss: 3.13505, obs_loss: 479.33813, reward_loss: 0.01369, value_loss: 0.18008 action_loss: -11.41456\n",
            "update_step:   7 model loss: 507.75635, kl_loss: 3.15001, obs_loss: 504.58371, reward_loss: 0.02261, value_loss: 0.16660 action_loss: -11.77837\n",
            "update_step:   8 model loss: 490.21274, kl_loss: 3.21300, obs_loss: 486.96429, reward_loss: 0.03542, value_loss: 0.20327 action_loss: -12.20965\n",
            "update_step:   9 model loss: 499.35904, kl_loss: 3.33234, obs_loss: 496.01019, reward_loss: 0.01652, value_loss: 0.18658 action_loss: -12.63032\n",
            "update_step:  10 model loss: 489.19443, kl_loss: 3.71335, obs_loss: 485.45917, reward_loss: 0.02192, value_loss: 0.18279 action_loss: -12.80556\n",
            "update_step:  11 model loss: 486.85303, kl_loss: 4.58408, obs_loss: 482.22729, reward_loss: 0.04165, value_loss: 0.22306 action_loss: -12.77901\n",
            "update_step:  12 model loss: 452.70004, kl_loss: 4.35074, obs_loss: 448.29089, reward_loss: 0.05841, value_loss: 0.28858 action_loss: -12.83547\n",
            "update_step:  13 model loss: 489.42194, kl_loss: 4.85082, obs_loss: 484.55106, reward_loss: 0.02005, value_loss: 0.35562 action_loss: -12.78275\n",
            "update_step:  14 model loss: 469.61719, kl_loss: 4.22171, obs_loss: 465.38831, reward_loss: 0.00719, value_loss: 0.39527 action_loss: -12.97623\n",
            "update_step:  15 model loss: 490.78552, kl_loss: 4.60386, obs_loss: 486.15332, reward_loss: 0.02834, value_loss: 0.37607 action_loss: -13.13835\n",
            "update_step:  16 model loss: 507.53644, kl_loss: 4.95867, obs_loss: 502.56689, reward_loss: 0.01086, value_loss: 0.35889 action_loss: -13.08230\n",
            "update_step:  17 model loss: 493.49338, kl_loss: 4.07288, obs_loss: 489.39697, reward_loss: 0.02352, value_loss: 0.34835 action_loss: -12.87953\n",
            "update_step:  18 model loss: 441.97995, kl_loss: 3.39177, obs_loss: 438.56403, reward_loss: 0.02417, value_loss: 0.28267 action_loss: -12.88393\n",
            "update_step:  19 model loss: 456.74203, kl_loss: 3.46143, obs_loss: 453.26129, reward_loss: 0.01933, value_loss: 0.21811 action_loss: -12.62938\n",
            "update_step:  20 model loss: 443.57510, kl_loss: 3.27863, obs_loss: 440.28381, reward_loss: 0.01265, value_loss: 0.15701 action_loss: -12.51970\n",
            "update_step:  21 model loss: 446.73474, kl_loss: 3.65832, obs_loss: 443.06097, reward_loss: 0.01544, value_loss: 0.13577 action_loss: -12.34538\n",
            "update_step:  22 model loss: 457.86685, kl_loss: 3.76298, obs_loss: 454.09439, reward_loss: 0.00948, value_loss: 0.14447 action_loss: -11.95885\n",
            "update_step:  23 model loss: 447.45834, kl_loss: 3.57619, obs_loss: 443.87189, reward_loss: 0.01027, value_loss: 0.15077 action_loss: -11.78007\n",
            "update_step:  24 model loss: 439.46329, kl_loss: 3.40482, obs_loss: 436.05161, reward_loss: 0.00687, value_loss: 0.16868 action_loss: -11.37107\n",
            "update_step:  25 model loss: 453.72934, kl_loss: 3.48765, obs_loss: 450.22232, reward_loss: 0.01938, value_loss: 0.17366 action_loss: -10.98512\n",
            "update_step:  26 model loss: 482.69244, kl_loss: 3.28633, obs_loss: 479.37860, reward_loss: 0.02752, value_loss: 0.16056 action_loss: -10.56868\n",
            "update_step:  27 model loss: 485.37216, kl_loss: 3.37305, obs_loss: 481.96271, reward_loss: 0.03640, value_loss: 0.12796 action_loss: -10.37617\n",
            "update_step:  28 model loss: 482.70908, kl_loss: 3.53767, obs_loss: 479.13574, reward_loss: 0.03567, value_loss: 0.10283 action_loss: -10.18662\n",
            "update_step:  29 model loss: 445.47455, kl_loss: 3.65517, obs_loss: 441.80667, reward_loss: 0.01270, value_loss: 0.08514 action_loss: -10.14781\n",
            "update_step:  30 model loss: 481.80334, kl_loss: 3.77214, obs_loss: 478.00586, reward_loss: 0.02533, value_loss: 0.08499 action_loss: -9.85022\n",
            "update_step:  31 model loss: 486.27652, kl_loss: 3.99214, obs_loss: 482.25964, reward_loss: 0.02474, value_loss: 0.10173 action_loss: -9.54444\n",
            "update_step:  32 model loss: 454.51044, kl_loss: 3.51536, obs_loss: 450.97235, reward_loss: 0.02274, value_loss: 0.10114 action_loss: -9.44206\n",
            "update_step:  33 model loss: 505.95502, kl_loss: 3.45383, obs_loss: 502.49521, reward_loss: 0.00598, value_loss: 0.12803 action_loss: -9.26427\n",
            "update_step:  34 model loss: 451.30023, kl_loss: 3.35519, obs_loss: 447.92877, reward_loss: 0.01626, value_loss: 0.12212 action_loss: -9.26864\n",
            "update_step:  35 model loss: 440.30518, kl_loss: 3.44564, obs_loss: 436.85135, reward_loss: 0.00818, value_loss: 0.10172 action_loss: -9.16182\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a650974e1a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/final-task/dreamer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobs_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0mmodel_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                 \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}